{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Mask Detector\n",
    "Author: [Handoyo](https://github.com/t3981-h) & [Tomy](https://github.com/tomytjandra)\n",
    "\n",
    "In this notebook, we will create a real-time face mask detector via video camera as follows.\n",
    "\n",
    "<img src=\"demo.gif\" width=\"500\">\n",
    "\n",
    "This application is created to automatically classify whether a face is covered with a mask or not. This is useful for today's COVID-19 situation because can be implemented on an automatic door lock to only allow those who have covered their face with a mask to enter. If implemented correctly, the mask detector we’re building here could potentially be used to ensure one's safety and the safety of others.\n",
    "\n",
    "First thing first, we train a Convolutional Neural Network using `keras` in order to \"teach\" the computer how to differentiate a facial feature of a covered face from the uncovered one. Then this trained model is applied after HaarCascade face detection on a video camera."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T06:26:34.701870Z",
     "start_time": "2020-06-29T06:26:00.279141Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Conv2D, Input, ZeroPadding2D, BatchNormalization, Activation, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling\n",
    "\n",
    "Here is the structure of our folder containing image data:\n",
    "\n",
    "    dataset\n",
    "    ├───test\n",
    "    │   ├───without_mask\n",
    "    │   └───with_mask\n",
    "    ├───train\n",
    "    │   ├───without_mask\n",
    "    │   └───with_mask\n",
    "    └───val\n",
    "        ├───without_mask\n",
    "        └───with_mask\n",
    "\n",
    "The folder `dataset` consists of three subfolders `test`, `train`, and `val` in which each of them has another subfolder: `without_mask` and `with_mask` denoting the class of our target variable.\n",
    "\n",
    "[Source of dataset](https://www.linkedin.com/feed/update/urn%3Ali%3Aactivity%3A6655711815361761280/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation\n",
    "\n",
    "We apply on-the-fly data augmentation, a technique to expand the `train` dataset size by creating a modified version of the original image which can improve model performance and the ability to generalize. This can be achieved by using `ImageDataGenerator()` provided by `keras`. We will not perform any data augmentation on the `test` data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T06:26:35.206269Z",
     "start_time": "2020-06-29T06:26:34.704432Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1315 images belonging to 2 classes.\n",
      "Found 194 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "TRAINING_DIR = \"dataset/train\"\n",
    "train_datagen = ImageDataGenerator(rescale=1.0/255,\n",
    "                                   rotation_range=40,\n",
    "                                   width_shift_range=0.2,\n",
    "                                   height_shift_range=0.2,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.2,\n",
    "                                   horizontal_flip=True,\n",
    "                                   fill_mode='nearest')\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(TRAINING_DIR, \n",
    "                                                    batch_size=20, \n",
    "                                                    target_size=(224, 224),\n",
    "                                                    seed=123)\n",
    "\n",
    "\n",
    "VALIDATION_DIR = \"dataset/test\"\n",
    "validation_datagen = ImageDataGenerator(rescale=1.0/255)\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(VALIDATION_DIR, \n",
    "                                                         batch_size=20, \n",
    "                                                         target_size=(150, 150),\n",
    "                                                         seed=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Network\n",
    "\n",
    "CNN is used as an automatic feature extractor from the images so that it can learn how to distinguish between `without_mask` and `with_mask` images. It effectively uses the adjacent pixel to downsample the image and then use a prediction (fully-connected) layer to solve the classification problem.\n",
    "\n",
    "Here is the detailed architecture that we are going to use:\n",
    "\n",
    "1. **First convolutional layer**: consists of 100 `filters` with `kernel_size` matrix 3 by 3.\n",
    "2. **First pooling layer**: Using max-pooling matrix 2 by 2 (`pool_size`) and 2-pixel `strides` at a time reduce the image size by half.\n",
    "3. **Second convolutional layer**: Same as the first convolutional layer.\n",
    "4. **Second pooling layer**: Same as the first pooling layer.\n",
    "5. **Flattening**: Convert two-dimensional pixel values into one dimension, so that it is ready to be fed into the fully-connected layer.\n",
    "6. **First dense layer + Dropout**: consists of 50 `units` and 1 bias unit. Dropout of rate 50% is used to prevent overfitting.\n",
    "7. **Output layer**: consists of two units and activation is a sigmoid function to convert the scores into a probability of an image being `without_mask` or `with_mask`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T06:26:36.201573Z",
     "start_time": "2020-06-29T06:26:35.209231Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 148, 148, 100)     2800      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 74, 74, 100)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 72, 72, 100)       90100     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 36, 36, 100)       0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 129600)            0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 129600)            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 50)                6480050   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 6,573,052\n",
      "Trainable params: 6,573,052\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(100, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(100, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    \n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(50, activation='relu'),\n",
    "    tf.keras.layers.Dense(2, activation='softmax')\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we specify how the model backpropagates or update the weights after each batch feed-forward. We use adam optimizer and a loss function binary cross-entropy since we are dealing with binary classification problem. The metrics used to monitor the training progress is accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "The full training process is available on [Google Colab](https://drive.google.com/file/d/1gw2CcVndcTTBRJXnu7NPrrLlEQXj2Gif/view?usp=sharing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-26T09:26:26.339971Z",
     "start_time": "2020-06-26T09:22:16.593528Z"
    }
   },
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint('model-{epoch:03d}.model',monitor='val_loss',verbose=0,save_best_only=True,mode='auto')\n",
    "history = model.fit_generator(train_generator,\n",
    "                              epochs=2 ,\n",
    "                              validation_data=validation_generator,\n",
    "                              callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the trained model instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-29T06:26:54.635724Z",
     "start_time": "2020-06-29T06:26:50.880703Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8500000238418579"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_saved = tf.keras.models.load_model('mask.h5')\n",
    "loss, acc = model_saved.evaluate(validation_generator, steps=3, verbose=0)\n",
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `model` achieves 85% accuracy for only two epochs of training, which is quite good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Turn On Your Camera\n",
    "\n",
    "Run the code below and test the face mask detector on your own. Enjoy! :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-26T12:11:56.845Z"
    }
   },
   "outputs": [],
   "source": [
    "labels_dict={0:'without_mask',1:'with_mask'}\n",
    "color_dict={0:(0,0,255),1:(0,255,0)}\n",
    "\n",
    "size = 4\n",
    "webcam = cv2.VideoCapture(0) # Use camera 0\n",
    "\n",
    "# We load the xml file, using HaarCascade for face detection\n",
    "classifier = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "while True:\n",
    "    (rval, im) = webcam.read()\n",
    "\n",
    "    # Flip to act as a mirror\n",
    "    im=cv2.flip(im,1,1)\n",
    "\n",
    "    # Resize the image to speed up detection\n",
    "    mini = cv2.resize(im, (im.shape[1] // size, im.shape[0] // size))\n",
    "\n",
    "    # Detect MultiScale / faces \n",
    "    faces = classifier.detectMultiScale(mini)\n",
    "\n",
    "    # Draw rectangles around each face\n",
    "    for f in faces:\n",
    "        # Scale the shapesize backup\n",
    "        (x, y, w, h) = [v * size for v in f]\n",
    "        \n",
    "        # Save just the rectangle faces in SubRecFaces\n",
    "        face_img = im[y:y+h, x:x+w]\n",
    "        resized=cv2.resize(face_img,(150,150))\n",
    "        normalized=resized/255.0\n",
    "        reshaped=np.reshape(normalized,(1,150,150,3))\n",
    "        reshaped = np.vstack([reshaped])\n",
    "        \n",
    "        # Classify using trained CNN\n",
    "        result=model_saved.predict(reshaped)\n",
    "        label=np.argmax(result,axis=1)[0]\n",
    "        \n",
    "        # Draw the rectangle\n",
    "        cv2.rectangle(im,(x,y),(x+w,y+h),color_dict[label],2)\n",
    "        cv2.rectangle(im,(x,y-40),(x+w,y),color_dict[label],-1)\n",
    "        cv2.putText(im, labels_dict[label], (x, y-10),cv2.FONT_HERSHEY_SIMPLEX,0.8,(255,255,255),2)\n",
    "        \n",
    "    # Show the image\n",
    "    cv2.imshow('LIVE', im)\n",
    "    key = cv2.waitKey(10)\n",
    "    \n",
    "    # If Esc key is press then break out of the loop \n",
    "    if key == 27: # Esc key\n",
    "        break\n",
    "        \n",
    "# Stop video\n",
    "webcam.release()\n",
    "\n",
    "# Close all started windows\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No pain no gain\n",
    "\n",
    "Good bye and see you again :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "computer-vision",
   "language": "python",
   "name": "computer-vision"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
